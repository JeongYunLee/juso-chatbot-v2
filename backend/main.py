# ê°œì„ ëœ ì„œë²„ ì½”ë“œ (main.py)

import uuid, os
import asyncio
import threading
from contextlib import asynccontextmanager
from dotenv import load_dotenv
from typing import TypedDict
from typing import Annotated

from openai import OpenAI
from langchain_openai import ChatOpenAI
import chromadb
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

from langchain_community.tools.tavily_search import TavilySearchResults
from langsmith import Client
from langchain_teddynote import logging
from langchain_core.tracers.context import collect_runs

from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from langgraph.errors import GraphRecursionError

from langchain_core.output_parsers import JsonOutputParser
from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableConfig  
from langchain_core.output_parsers import StrOutputParser

from langchain_community.chat_message_histories import ChatMessageHistory, StreamlitChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langgraph.graph.message import add_messages
from operator import itemgetter

from langchain.agents import tool
from langchain.agents import create_tool_calling_agent
from langchain.agents import AgentExecutor

from langchain_teddynote import logging

# .env íŒŒì¼ í™œì„±í™” & API KEY ì„¤ì •
load_dotenv(override=True)
openai_api_key = os.getenv('OPENAI_API_KEY')
logging.langsmith("hike-jusochatbot-demo")

llm_4o = ChatOpenAI(model="gpt-4o", temperature=0)

class GraphState(TypedDict):
    question: str # ì§ˆë¬¸
    q_type: str  # ì§ˆë¬¸ì˜ ìœ í˜•
    context: list | str  # ë¬¸ì„œì˜ ê²€ìƒ‰ ê²°ê³¼
    answer: str | list[str]   # llmì´ ìƒì„±í•œ ë‹µë³€
    relevance: str  # ë‹µë³€ì˜ ë¬¸ì„œì— ëŒ€í•œ ê´€ë ¨ì„± (groundness check)
    session_id: str  # ì„¸ì…˜ ID ì¶”ê°€

# ğŸ”§ ê°œì„  1: ìŠ¤ë ˆë“œ ì•ˆì „í•œ ì €ì¥ì†Œ
import threading
from collections import defaultdict

class ThreadSafeStore:
    def __init__(self):
        self._store = {}
        self._lock = threading.RLock()  # ì¬ì§„ì… ê°€ëŠ¥í•œ ë½
    
    def get_session_history(self, session_id: str):
        with self._lock:
            if session_id not in self._store:
                self._store[session_id] = ChatMessageHistory()
                print(f"ğŸ†• ìƒˆë¡œìš´ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ìƒì„±: {session_id[:8]}...")
            return self._store[session_id]
    
    def clear_session(self, session_id: str = None):
        with self._lock:
            if session_id:
                if session_id in self._store:
                    message_count = len(self._store[session_id].messages)
                    del self._store[session_id]
                    return message_count
                return 0
            else:
                total_sessions = len(self._store)
                total_messages = sum(len(history.messages) for history in self._store.values())
                self._store.clear()
                return total_sessions, total_messages
    
    def get_stats(self):
        with self._lock:
            return {
                'total_sessions': len(self._store),
                'total_messages': sum(len(history.messages) for history in self._store.values())
            }

# ì „ì—­ ìŠ¤ë ˆë“œ ì•ˆì „ ì €ì¥ì†Œ
thread_safe_store = ThreadSafeStore()

# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_ids):
    return thread_safe_store.get_session_history(session_ids)

# ìƒˆë¡œìš´ ì„¸ì…˜ ID ìƒì„± í•¨ìˆ˜
def generate_session_id():
    return str(uuid.uuid4())

#######################################################################
############################ nodes: Router ############################
#######################################################################

class Router(BaseModel):
    type: str = Field(description="type of the query that model choose")

router_output_parser = JsonOutputParser(pydantic_object=Router)
format_instructions = router_output_parser.get_format_instructions()

router_prompt = PromptTemplate(
    template="""
            You are an expert who classifies the type of question. There are two query types: ['general', 'domain_specific']

            [general]
            Questions unrelated to addresses, such as translating English to Korean, asking for general knowledge (e.g., "What is the capital of South Korea?"), or queries that can be answered through a web search.

            [domain_specific]
            Questions related to addresses, such as concepts, definitions, address-related data analysis, or reviewing properly written addresses (e.g., "ìˆ˜ì§€êµ¬ëŠ” ìì¹˜êµ¬ì´ë‹ˆ ì¼ë°˜êµ¬ì´ë‹ˆ?", "íŠ¹ë³„ì‹œì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜", "ì£¼ì†Œì™€ ì£¼ì†Œì •ë³´ì˜ ì°¨ì´ì ì€?").

            <Output format>: Always respond with either "general" or "domain_specific" and nothing else. {format_instructions}
            <chat_history>: {chat_history}
            
            <Question>: {query} 
            """,
    input_variables=["query", "chat_history"],
    partial_variables={"format_instructions": format_instructions},
)

def router(state: GraphState) -> GraphState:
    chain = router_prompt | llm_4o | router_output_parser
    
    router_with_history  = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="query",
        history_messages_key="chat_history",
    )
    
    router_result = router_with_history.invoke(
        {"query": state["question"]}, 
        {'configurable': {'session_id': state["session_id"]}}
    )
    state["q_type"] = router_result['type']
    return state

def router_conditional_edge(state: GraphState) -> GraphState:
    q_type = state["q_type"].strip()
    return q_type

##################################################################################
############################ nodes: Retrieve Document ############################
##################################################################################

# ğŸ”§ ê°œì„  2: ChromaDB ì—°ê²° í’€ë§ ë° ì¬ì‹œë„ ë¡œì§
import time
from functools import wraps

def retry_on_failure(max_retries=3, delay=1):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ ì‹œë„ {attempt + 1} ì‹¤íŒ¨, {delay}ì´ˆ í›„ ì¬ì‹œë„: {str(e)[:100]}")
                        time.sleep(delay * (attempt + 1))  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    else:
                        print(f"âŒ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨: {str(e)}")
            raise last_exception
        return wrapper
    return decorator

# ChromaDB í´ë¼ì´ì–¸íŠ¸ë¥¼ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ìƒì„±í•˜ì—¬ ì¶©ëŒ ë°©ì§€
@retry_on_failure(max_retries=3, delay=1)
def get_vectorstore():
    try:
        client = chromadb.PersistentClient('chroma/')
        embedding = OpenAIEmbeddings(model='text-embedding-3-large')  
        vectorstore = Chroma(client=client, collection_name="49_files_openai_3072", embedding_function=embedding)
        return vectorstore
    except Exception as e:
        print(f"ChromaDB ì—°ê²° ì‹¤íŒ¨: {e}")
        raise

def retrieve_document(state: GraphState) -> GraphState:
    try:
        vectorstore = get_vectorstore()
        retrieved_docs_with_score = vectorstore.similarity_search_with_score(state["question"], k=3)

        serialized_docs = [
            {
                "page_content": doc.page_content,
                "metadata": doc.metadata,
                "score": score
            }
            for doc, score in retrieved_docs_with_score
        ]

        return {**state, "context": serialized_docs}
    except Exception as e:
        print(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        # ë¹ˆ ì»¨í…ìŠ¤íŠ¸ë¡œ ê³„ì† ì§„í–‰
        return {**state, "context": []}

#########################################################################
############################ nodes: Verifier ############################
#########################################################################

class Verifier(BaseModel):
    type: str = Field(description="verify that retrieved data is sufficient to answer the query")

verifier_output_parser = JsonOutputParser(pydantic_object=Verifier)
format_instructions = verifier_output_parser.get_format_instructions()

verifier_prompt = PromptTemplate(
    template="""
            You are an expert who verity the retrieved data's quality and usefullness to answer the query. There are two query types: ['sufficient', 'insufficient', 'unsuitable']

            [sufficient]
            When the retrieved data is sufficient to answer the query.
            
            [insufficient]
            When the retrieved data is insufficient to answer the query, triggering additional actions or tool usage:
            1.	When the context information is inadequate to respond to the query, requiring further steps (e.g., search).
            2.	When the query involves tasks beyond simple address-related information retrieval, such as report generation or image creation.

            [unsuitable]
            When the retrieved data is not suitable to answer the query.
            
            <Output format>: Always respond with either "sufficient", "insufficient" or "unsuitable" and nothing else. {format_instructions}
            
            <Question>: {query} 
            <Retrieved data>: {retrieved_data}
            """,
    input_variables=["query", "retrieved_data"],
    partial_variables={"format_instructions": format_instructions},
)

def verifier(state: GraphState) -> GraphState:
    chain = verifier_prompt | llm_4o | verifier_output_parser
    verified = chain.invoke(
        {"query": state["question"], "retrieved_data": state["context"]}, 
        {'configurable': {'session_id': state["session_id"]}}
    )
    state["relevance"] = verified['type']
    return state

def verifier_conditional_edge(state: GraphState) -> str:
    verified_result = state["relevance"].strip()
    
    if verified_result not in ["sufficient", "insufficient", "unsuitable"]:
        raise ValueError(f"Unexpected verifier result: {verified_result}")

    return verified_result
 
############################ tools ############################

# ğŸ”§ ê°œì„  3: OpenAI API ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… ë° ì¬ì‹œë„
import openai
from openai import RateLimitError, APITimeoutError

@retry_on_failure(max_retries=3, delay=2)
def call_openai_with_retry(client, **kwargs):
    try:
        return client.chat.completions.create(**kwargs)
    except RateLimitError as e:
        print(f"âš ï¸ OpenAI ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸: {e}")
        time.sleep(5)  # ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ì‹œ ë” ì˜¤ë˜ ëŒ€ê¸°
        raise
    except APITimeoutError as e:
        print(f"âš ï¸ OpenAI íƒ€ì„ì•„ì›ƒ: {e}")
        raise
    except Exception as e:
        print(f"âš ï¸ OpenAI API ì˜¤ë¥˜: {e}")
        raise

@tool
def search_on_web(input):
    """ ì‹¤ì‹œê°„ ì •ë³´, ìµœì‹  ì •ë³´ ë“± ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë„êµ¬ """
    try:
        search_tool = TavilySearchResults(max_results=5)
        search_result = search_tool.invoke({"query": input})
        return search_result
    except Exception as e:
        print(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return "ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper
from langchain.tools import tool

dalle = DallEAPIWrapper(model="dall-e-3", size="1024x1024", quality="standard", n=1)

@tool 
def dalle_tool(query):
    """use this tool to generate image from text"""
    try:
        return dalle.run(query)
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
        return "ì´ë¯¸ì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

@tool
def advanced_assistant(input, retrieved_data):
    """ ê³ ê¸‰ ê¸°ëŠ¥(ì˜ˆ: ë³´ê³ ì„œ ìƒì„±, ê¸´ ë¬¸ì„œ ìƒì„±, ì¶”ë¡ ì´ í•„ìš”í•œ ë‹µë³€ ë“±)ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ëª¨ë¸ """
    try:
        client = OpenAI()
        
        response = call_openai_with_retry(
            client,
            model="gpt-4o",  # o3 ëŒ€ì‹  ë” ì•ˆì •ì ì¸ gpt-4o ì‚¬ìš©
            messages=[
                { "role": "developer", "content": "You are a helpful assistant." },
                {
                    "role": "user", 
                    "content": f"query: {input}\n\n retrieved data: {retrieved_data}"
                }
            ],
            timeout=60  # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        )
        
        result = response.choices[0].message.content
        return result
    except Exception as e:
        print(f"ê³ ê¸‰ ì§€ì› ì‹¤íŒ¨: {e}")
        return "ê³ ê¸‰ ê¸°ëŠ¥ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

@tool
def image_explainer(query, image_url):
    """ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±ê¸°. Use after the image_generator tool make the image output. Use the url of the image_generator tool. """
    try:
        client = OpenAI()

        response = call_openai_with_retry(
            client,
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": query},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image_url,
                            },
                        },
                    ],
                }
            ],
            max_tokens=300,
            timeout=60
        )
        return response.choices[0]
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ì„¤ëª… ì‹¤íŒ¨: {e}")
        return "ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
tools = [search_on_web, dalle_tool, advanced_assistant, image_explainer]

agent_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant used Korean ONLY. "
            "When `Relevance` is sufficient, you may provide the answer directly. "
            "When `Relevance` is insufficient, you MUST use another tools like search_on_web or advanced_assistant. "
            "If you use `Retrieved Data`, you must state Cites from the reference on the bottom of your answer. "
            "If the information from pdf_search tool is insufficient, you can find further or recent information by using search tool. "
            "If you use search tool, you can add href link. "
            "You can use image generation tool to generate image from text. "
            "You can use advanced_assistant to write long report or reasoning tasks."
        ),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("human", "{retrieved_data}"),
        ("placeholder", "{agent_scratchpad}"),
    ]
)

def agent(state: GraphState) -> GraphState:
    try:
        llm = ChatOpenAI(model="gpt-4o", temperature=0, timeout=60)  # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        agent = create_tool_calling_agent(llm, tools, agent_prompt)
        
        agent_executor = AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=False,
            max_iterations=10,  # ë°˜ë³µ íšŸìˆ˜ ì¤„ì„
            max_execution_time=120,  # ì‹¤í–‰ ì‹œê°„ ì œí•œ
            handle_parsing_errors=True,
            return_intermediate_steps=True
        )

        agent_with_history = RunnableWithMessageHistory(
            agent_executor,
            get_session_history,
            history_messages_key="chat_history",
        )

        result = agent_with_history.invoke(
            {"input": state["question"], "retrieved_data": state["context"], "relevance": state["relevance"]}, 
            {'configurable': {'session_id': state["session_id"]}}
        )
        state['answer'] = result['output']

        return state
    except Exception as e:
        print(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        state['answer'] = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)[:100]}"
        return state

########################################################################
############################ Workflow Graph ############################
########################################################################

workflow = StateGraph(GraphState)

workflow.add_node("Router", router)
workflow.add_node("Retrieved Data", retrieve_document)
workflow.add_node("Agent", agent)
workflow.add_node("Verifier", verifier)

workflow.add_conditional_edges(
    "Router",
    router_conditional_edge,
    {"domain_specific": "Retrieved Data",  "general": "Agent"},
)

workflow.add_edge("Retrieved Data", "Verifier")
workflow.add_edge("Verifier", "Agent")
workflow.add_edge("Agent", END)

workflow.set_entry_point("Router")

memory = MemorySaver()
graph = workflow.compile(checkpointer=memory)    

##############################################################################################################
################################################Chat Interface################################################
##############################################################################################################

from fastapi import FastAPI, Request, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel

# ğŸ”§ ê°œì„  4: FastAPI ì•± ìƒì„± ì‹œ lifespan ì´ë²¤íŠ¸ ì¶”ê°€
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ ì‹œ
    print("ğŸš€ ì„œë²„ ì‹œì‘")
    yield
    # ì¢…ë£Œ ì‹œ
    print("ğŸ›‘ ì„œë²„ ì¢…ë£Œ")

app = FastAPI(title="Juso Chatbot API", lifespan=lifespan)

from pydantic_settings import BaseSettings
from functools import lru_cache
import os
from dotenv import load_dotenv

load_dotenv()

class Settings(BaseSettings):
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
    ALLOWED_ORIGINS: list = [
        "http://localhost:3000",
        "http://localhost:3001", 
        "https://localhost:3000",
        "https://localhost:3001",
        "http://labs.datahub.kr",
        "https://labs.datahub.kr",
        "http://localhost:8000",
        "https://localhost:8000",
    ]

@lru_cache()
def get_settings():
    return Settings() 

settings = get_settings()

app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class MessageRequest(BaseModel):
    message: str
    session_id: str = None

class FeedbackRequest(BaseModel):
    score: float
    run_id: str

current_user_id = None

# ğŸ”§ ê°œì„  5: ë¹„ë™ê¸° ì²˜ë¦¬ ë° ìƒì„¸í•œ ì—ëŸ¬ í•¸ë“¤ë§
@app.post("/api/")
async def stream_responses(request: Request):
    try:
        data = await request.json()
        message = data.get('message')
        client_session_id = data.get('session_id')
        
        if not message:
            raise HTTPException(status_code=400, detail="Message is required")
        
        if not message.strip():
            raise HTTPException(status_code=400, detail="Message cannot be empty")

        # ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ
        if len(message) > 1000:
            raise HTTPException(status_code=400, detail="Message too long (max 1000 characters)")

        # ì„¸ì…˜ ID ì²˜ë¦¬
        if not client_session_id:
            client_session_id = generate_session_id()

        # ğŸ”§ ê°œì„  6: ì„¤ì • ìµœì í™”
        config = RunnableConfig(
            recursion_limit=10,  # ì¬ê·€ ì œí•œ ì¤„ì„
            configurable={
                "thread_id": f"HIKE-JUSOCHATBOT-{client_session_id[:8]}", 
                "user_id": current_user_id, 
                "session_id": client_session_id
            }
        )

        inputs = GraphState(
            question=message,
            session_id=client_session_id,
            q_type='',
            context='',
            answer='',
            relevance='',
        )

        try:
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •ìœ¼ë¡œ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
            final_state = await asyncio.wait_for(
                asyncio.to_thread(graph.invoke, inputs, config),
                timeout=180  # 3ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            answer_text = final_state["answer"]
            
            # ì‘ë‹µ ê²€ì¦
            if not answer_text or not isinstance(answer_text, str):
                answer_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ì„¸ì…˜ í†µê³„
            current_history = get_session_history(client_session_id)
            message_count = len(current_history.messages)
            
            print(f"âœ… ì„¸ì…˜ {client_session_id[:8]}... ì‘ë‹µ ì™„ë£Œ (ì´ {message_count}ê°œ ë©”ì‹œì§€)")
            
            return {
                "answer": answer_text,
                "session_id": client_session_id,
                "message_count": message_count,
                "status": "success"
            }
            
        except asyncio.TimeoutError:
            print(f"â° íƒ€ì„ì•„ì›ƒ: {client_session_id[:8]}...")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                "session_id": client_session_id,
                "status": "timeout"
            }
        except GraphRecursionError as e:
            print(f"ğŸ”„ ì¬ê·€ ì œí•œ ì´ˆê³¼: {e}")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì´ ë„ˆë¬´ ë³µì¡í•©ë‹ˆë‹¤. ë” ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                "session_id": client_session_id,
                "status": "recursion_error"
            }
        except Exception as e:
            print(f"âŒ ê·¸ë˜í”„ ì‹¤í–‰ ì˜¤ë¥˜: {type(e).__name__}: {str(e)[:200]}")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                "session_id": client_session_id,
                "status": "error",
                "error_type": type(e).__name__
            }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ API ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.post("/api/reset")
async def reset_store(request: Request):
    try:
        data = await request.json()
        session_id_to_reset = data.get('session_id')
        
        if session_id_to_reset:
            # íŠ¹ì • ì„¸ì…˜ë§Œ ì´ˆê¸°í™”
            message_count = thread_safe_store.clear_session(session_id_to_reset)
            new_session_id = generate_session_id()
            
            print(f"ğŸ—‘ï¸ ì„¸ì…˜ ì‚­ì œ: {session_id_to_reset[:8]}... ({message_count}ê°œ ë©”ì‹œì§€)")
            
            return {
                "status": "Session reset successfully",
                "session_id": new_session_id,
                "cleared_messages": message_count
            }
        else:
            # ëª¨ë“  ì„¸ì…˜ ì´ˆê¸°í™”
            total_sessions, total_messages = thread_safe_store.clear_session()
            new_session_id = generate_session_id()
            
            print(f"ğŸ§¹ ì „ì²´ ì´ˆê¸°í™”: {total_sessions}ê°œ ì„¸ì…˜, {total_messages}ê°œ ë©”ì‹œì§€ ì‚­ì œ")
            
            return {
                "status": "All sessions reset successfully",
                "session_id": new_session_id,
                "cleared_sessions": total_sessions,
                "cleared_messages": total_messages
            }
            
    except Exception as e:
        print(f"âŒ ë¦¬ì…‹ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒì‹œì—ë„ ìƒˆ ì„¸ì…˜ ID ë°˜í™˜
        new_session_id = generate_session_id()
        
        return {
            "status": "Sessions reset due to error",
            "session_id": new_session_id,
            "error": str(e)
        }

# ğŸ”§ ê°œì„  7: í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@app.get("/health")
async def health_check():
    stats = thread_safe_store.get_stats()
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "sessions": stats['total_sessions'],
        "messages": stats['total_messages']
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app", 
        host="0.0.0.0", 
        port=8000, 
        reload=True,
        workers=1,  # ë‹¨ì¼ ì›Œì»¤ë¡œ ë©”ëª¨ë¦¬ ê³µìœ  ë¬¸ì œ ë°©ì§€
        timeout_keep_alive=30,
        limit_concurrency=100,  # ë™ì‹œ ì—°ê²° ì œí•œ
        limit_max_requests=1000  # ìµœëŒ€ ìš”ì²­ ìˆ˜ ì œí•œ
    )